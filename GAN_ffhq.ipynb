{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, BatchNormalization\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, img_width, img_height, img_channels, DM_optimizer = RMSprop(lr = 0.0002, decay = 6e-8),\\\n",
    "                 AM_optimizer = RMSprop(lr = 0.0001, decay = 3e-8), input_noice_dim = 100, print_summary = False):\n",
    "        \n",
    "        self.input_shape = (img_width, img_height, img_channels)\n",
    "        self.input_noice_dim = input_noice_dim\n",
    "        \n",
    "        if print_summary:\n",
    "            print(\"Generator:\")\n",
    "        self.Gen = self.__generator(print_summary)\n",
    "        \n",
    "        if print_summary:\n",
    "            print(\"\\nDiscriminator:\")\n",
    "        self.Dis = self.__discriminator(print_summary)\n",
    "        \n",
    "        if print_summary:\n",
    "            print(\"\\nDM:\")\n",
    "        self.DM = self.__dm(DM_optimizer, print_summary)\n",
    "        \n",
    "        if print_summary:\n",
    "            print(\"\\nAM:\")\n",
    "        self.AM = self.__am(AM_optimizer, print_summary)\n",
    "        \n",
    "    def __generator(self, print_summary = False):\n",
    "        g = Sequential(name = 'generator')\n",
    "        dropout = 0.5\n",
    "        depth = 256\n",
    "        dim = int(self.input_shape[0] / 4)\n",
    "        \n",
    "        g.add(Dense(dim * dim * depth, input_dim = self.input_noice_dim))\n",
    "        g.add(BatchNormalization(momentum = 0.99))\n",
    "        g.add(LeakyReLU(alpha = 0.2))\n",
    "        g.add(Reshape((dim, dim, depth)))\n",
    "        g.add(Dropout(dropout))\n",
    "        \n",
    "        g.add(UpSampling2D())\n",
    "        g.add(Conv2DTranspose(int(depth / 2), 5, padding = 'same'))\n",
    "        g.add(BatchNormalization(momentum = 0.99))\n",
    "        g.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        g.add(UpSampling2D())\n",
    "        g.add(Conv2DTranspose(int(depth / 4), 5, padding = 'same'))\n",
    "        g.add(BatchNormalization(momentum = 0.99))\n",
    "        g.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        g.add(Conv2DTranspose(int(depth / 8), 5, padding = 'same'))\n",
    "        g.add(BatchNormalization(momentum = 0.99))\n",
    "        g.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        g.add(Conv2DTranspose(self.input_shape[2], 5, padding = 'same'))\n",
    "        g.add(Activation('tanh')) #tu by≈Ç sigmoid\n",
    "        \n",
    "        if print_summary:\n",
    "            g.summary()\n",
    "\n",
    "        return g\n",
    "    \n",
    "    def __discriminator(self, print_summary = False):\n",
    "        d = Sequential(name = \"discriminator\")\n",
    "        dropout = 0.5\n",
    "        depth = 64\n",
    "        \n",
    "        d.add(Conv2D(depth * 1, 5, strides = 2, input_shape = self.input_shape, padding = 'same'))\n",
    "        d.add(LeakyReLU(alpha = 0.2))\n",
    "        d.add(Dropout(dropout))\n",
    "        \n",
    "        d.add(Conv2D(depth * 2, 5, strides = 2, padding = 'same'))\n",
    "        d.add(LeakyReLU(alpha = 0.2))\n",
    "        d.add(Dropout(dropout))\n",
    "        \n",
    "        d.add(Conv2D(depth * 4, 5, strides = 2, padding = 'same'))\n",
    "        d.add(LeakyReLU(alpha = 0.2))\n",
    "        d.add(Dropout(dropout))\n",
    "        \n",
    "        d.add(Conv2D(depth * 16, 5, strides = 1, padding = 'same'))\n",
    "        d.add(LeakyReLU(alpha = 0.2))\n",
    "        d.add(Dropout(dropout))\n",
    "        \n",
    "        d.add(Flatten())\n",
    "        d.add(Dense(1))\n",
    "        d.add(Activation('sigmoid'))\n",
    "        \n",
    "        if print_summary:\n",
    "            d.summary()\n",
    "            \n",
    "        return d\n",
    "    \n",
    "    def __dm(self, optimizer, print_summary = False):\n",
    "        self.Dis.trainable = True\n",
    "        dm = Sequential()\n",
    "        dm.add(self.Dis)\n",
    "        \n",
    "        if print_summary:\n",
    "            dm.summary()\n",
    "        \n",
    "        dm.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "        \n",
    "        return dm\n",
    "    \n",
    "    def __am(self, optimizer, print_summary = False):\n",
    "        #Freezing Discriminator weights during the generator adversarial training\n",
    "        self.Dis.trainable = False\n",
    "        am = Sequential()\n",
    "        am.add(self.Gen)\n",
    "        am.add(self.Dis)\n",
    "        \n",
    "        if print_summary:\n",
    "            am.summary()\n",
    "        \n",
    "        am.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "        \n",
    "        return am\n",
    "    \n",
    "    def train(self, steps, batch_size, dataset, info_rate = 10):\n",
    "        for i in range(steps):\n",
    "            r = dataset.get_random_batch(batch_size)\n",
    "            noise = np.random.normal(0, 0.5, size = [batch_size, self.input_noice_dim])\n",
    "            f = self.Gen.predict(noise)\n",
    "            x = np.concatenate((r, f))\n",
    "            y = np.ones([2 * batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            dm_loss = self.DM.train_on_batch(x, y)\n",
    "            \n",
    "            y = np.ones([2 * batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[2 * batch_size, self.input_noice_dim])\n",
    "            am_loss = self.AM.train_on_batch(noise, y)\n",
    "            \n",
    "            if i % info_rate == 0:\n",
    "                print(\"Step:\", i, \"DM loss:\", dm_loss, \"AM loss:\", am_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffhq_dirs = glob(os.path.join('.', 'ffhq_imgs_thumbnails', '*.png'))\n",
    "ffhq_dataset = helper.ImagesDataset(ffhq_dirs, 'RGB', lambda x, **kw: x.resize([kw['width'], kw['height']], Image.BILINEAR), width=56, height=56)\n",
    "print(ffhq_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00025\n",
    "beta1 = 0.45\n",
    "\n",
    "face_cnn = GAN(56, 56, 3, DM_optimizer = Adam(lr = learning_rate, beta_1 = beta1),\\\n",
    "               AM_optimizer = Adam(lr = learning_rate, beta_1 = beta1), input_noice_dim = 400, print_summary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cnn.train(30000, 16, ffhq_dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 0.5, size = [1, face_cnn.input_noice_dim])\n",
    "i = face_cnn.Gen.predict(noise)\n",
    "print(face_cnn.Dis.predict(i))\n",
    "#i = (((i - i.min()) * 255) / (i.max() - i.min())).astype(np.uint8)\n",
    "plt.imshow(helper.image_for_plot(i)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cnn.AM.save('face_gen_little_deeper.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_upsampled = tf.keras.backend.resize_images(image_scaled.reshape([1, 28, 28, 3]), 4, 4, \"channels_last\")\n",
    "print(image_upsampled.shape)\n",
    "print(image_upsampled)\n",
    "plt.imshow(tf.to_float(image_upsampled[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "us = Upsampler(4)\n",
    "\n",
    "image_upsampled = us.upsample(image_scaled.reshape([1, 28, 28, 3]))\n",
    "print(image_upsampled.shape)\n",
    "plt.imshow(image_upsampled[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
